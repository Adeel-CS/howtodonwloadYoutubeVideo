{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adeel-CS/howtodonwloadYoutubeVideo/blob/main/Copy_of_FYP_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "finalizing at this point"
      ],
      "metadata": {
        "id": "WZL00tnBxJjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from pytube import YouTube\n",
        "import whisper\n",
        "import moviepy.editor as mp\n",
        "from transformers import pipeline\n",
        "from keybert import KeyBERT\n",
        "\n",
        "# Function to check and install packages\n",
        "def install_package(package):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"Installed {package} successfully!\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"Error installing {package}. Please check your internet connection or try again later.\")\n",
        "\n",
        "# Install required packages if not already installed\n",
        "required_packages = [\"pytube\", \"whisper\", \"moviepy\", \"transformers\", \"keybert\", \"huggingface_hub\"]\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        install_package(package)\n",
        "\n",
        "# Function to sanitize URLs for filenames\n",
        "def sanitize_filename(url):\n",
        "    return re.sub(r'\\W+', '_', url)\n",
        "\n",
        "# Function to download video from YouTube\n",
        "def download_video(video_url, download_path):\n",
        "    try:\n",
        "        yt = YouTube(video_url)\n",
        "        stream = yt.streams.get_highest_resolution()\n",
        "        video_file = stream.download(output_path=download_path)\n",
        "        print(\"Download complete!\")\n",
        "        return video_file\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to extract audio from video\n",
        "def extract_audio(video_file, audio_file):\n",
        "    try:\n",
        "        video = mp.VideoFileClip(video_file)\n",
        "        video.audio.write_audiofile(audio_file)\n",
        "        print(\"Audio extraction complete!\")\n",
        "        return audio_file\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during audio extraction: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to transcribe audio using Whisper\n",
        "def transcribe_audio(audio_file):\n",
        "    try:\n",
        "        model = whisper.load_model(\"base\")\n",
        "        result = model.transcribe(audio_file)\n",
        "        print(\"Transcription complete!\")\n",
        "        formatted_text = result[\"text\"].replace(\". \", \".\\n\")\n",
        "        return formatted_text\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during transcription: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to extract key topics using KeyBERT\n",
        "def extract_key_topics(text, num_topics=10):\n",
        "    try:\n",
        "        kw_model = KeyBERT()\n",
        "        key_topics = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=num_topics)\n",
        "        topics = [keyword for keyword, _ in key_topics]\n",
        "        print(\"Key topics extraction complete!\")\n",
        "        return topics\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during key topics extraction: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to summarize text using transformers pipeline\n",
        "def summarize_text(text, max_chunk_length=1024):\n",
        "    try:\n",
        "        summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "        sentences = text.split('. ')\n",
        "        current_chunk = []\n",
        "        chunks = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if len(current_chunk) + len(sentence.split()) <= max_chunk_length:\n",
        "                current_chunk.append(sentence)\n",
        "            else:\n",
        "                chunks.append('. '.join(current_chunk) + '.')\n",
        "                current_chunk = [sentence]\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append('. '.join(current_chunk) + '.')\n",
        "\n",
        "        summary = []\n",
        "        for chunk in chunks:\n",
        "            chunk_length = len(chunk.split())\n",
        "            max_new_tokens = min(512, chunk_length + 100)\n",
        "            min_length = min(150, chunk_length // 2)\n",
        "            chunk_summary = summarizer(chunk, max_new_tokens=max_new_tokens, min_length=min_length, do_sample=False)\n",
        "            summary.append(chunk_summary[0]['summary_text'])\n",
        "\n",
        "        combined_summary = ' '.join(summary)\n",
        "        print(\"Summarization complete!\")\n",
        "        return combined_summary\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during summarization: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "#To remove the warning Your min_length=150 must be inferior than your max_length=142. but we will be getting smaller summary need to adjust this thing\n",
        "# def summarize_text(text, max_chunk_length=1024):\n",
        "#     try:\n",
        "#         summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "#         sentences = text.split('. ')\n",
        "#         current_chunk = []\n",
        "#         chunks = []\n",
        "\n",
        "#         for sentence in sentences:\n",
        "#             if len(current_chunk) + len(sentence.split()) <= max_chunk_length:\n",
        "#                 current_chunk.append(sentence)\n",
        "#             else:\n",
        "#                 chunks.append('. '.join(current_chunk) + '.')\n",
        "#                 current_chunk = [sentence]\n",
        "\n",
        "#         if current_chunk:\n",
        "#             chunks.append('. '.join(current_chunk) + '.')\n",
        "\n",
        "#         summary = []\n",
        "#         for chunk in chunks:\n",
        "#             chunk_length = len(chunk.split())\n",
        "#             max_new_tokens = min(150, chunk_length + 20)  # Slightly increase max_new_tokens to handle edge cases\n",
        "#             min_length = min(30, chunk_length // 2)\n",
        "#             chunk_summary = summarizer(chunk, max_new_tokens=max_new_tokens, min_length=min_length, do_sample=False)\n",
        "#             summary.append(chunk_summary[0]['summary_text'])\n",
        "\n",
        "#         combined_summary = ' '.join(summary)\n",
        "#         print(\"Summarization complete!\")\n",
        "#         return combined_summary\n",
        "#     except Exception as e:\n",
        "#         print(f\"An error occurred during summarization: {e}\")\n",
        "#         return None\n"
      ],
      "metadata": {
        "id": "atIUUN_btVyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    video_url = input(\"Enter the video URL: \")\n",
        "    sanitized_url = sanitize_filename(video_url)\n",
        "\n",
        "    download_path = \"./downloads\"\n",
        "    audio_path = \"./audio\"\n",
        "    os.makedirs(download_path, exist_ok=True)\n",
        "    os.makedirs(audio_path, exist_ok=True)\n",
        "\n",
        "    video_file = download_video(video_url, download_path)\n",
        "\n",
        "    if video_file:\n",
        "        audio_file = os.path.join(audio_path, f\"audio_{sanitized_url}.mp3\")\n",
        "        audio_file = extract_audio(video_file, audio_file)\n",
        "\n",
        "        if audio_file:\n",
        "            transcription = transcribe_audio(audio_file)\n",
        "\n",
        "            if transcription:\n",
        "                transcription_file = f\"transcription_{sanitized_url}.txt\"\n",
        "                with open(transcription_file, \"w\") as f:\n",
        "                    f.write(transcription)\n",
        "                print(f\"Transcription saved to {transcription_file}\")\n",
        "\n",
        "                key_topics = extract_key_topics(transcription)\n",
        "\n",
        "                if key_topics:\n",
        "                    key_topics_file = f\"key_topics_{sanitized_url}.txt\"\n",
        "                    with open(key_topics_file, \"w\") as f:\n",
        "                        f.write(\"\\n\".join(key_topics))\n",
        "                    print(f\"Key topics saved to {key_topics_file}\")\n",
        "\n",
        "                summary = summarize_text(transcription)\n",
        "\n",
        "                if summary:\n",
        "                    summary_file = f\"summary_{sanitized_url}.txt\"\n",
        "                    with open(summary_file, \"w\") as f:\n",
        "                        f.write(summary)\n",
        "                    print(f\"Summary saved to {summary_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAzRM1OqusVD",
        "outputId": "7b52460d-2660-49d7-dfee-5c6d8a7dd08a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the video URL: https://www.youtube.com/watch?v=5sLYAQS9sWQ\n",
            "Download complete!\n",
            "MoviePy - Writing audio in ./audio/audio_https_www_youtube_com_watch_v_5sLYAQS9sWQ.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Audio extraction complete!\n",
            "Transcription complete!\n",
            "Transcription saved to transcription_https_www_youtube_com_watch_v_5sLYAQS9sWQ.txt\n",
            "Key topics extraction complete!\n",
            "Key topics saved to key_topics_https_www_youtube_com_watch_v_5sLYAQS9sWQ.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your min_length=150 must be inferior than your max_length=142.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarization complete!\n",
            "Summary saved to summary_https_www_youtube_com_watch_v_5sLYAQS9sWQ.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ckL8G14Ruv2D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}